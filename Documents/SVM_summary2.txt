SVM

	1. Hyper plane
	2. Hard margin / Soft margin
		very sensetive to outliers
	3. Support vectors : 
		3.1 	closest vector to hyperPlane
		3.2 	widest street margin
	4. Kernel Trick
	5. Optimization problem : SVM finds the widest  street between the nearest points on either side.
		5.1 Margin violation
		5.2 Penalty = C * | Margin violation | ( C - high = hard margin)
		5.3 Best desicion boundry seeks to : maximize width of the street , minimize the margin violations. 
		
		
	6. features | target variable
	

	Model evaluation:
		1. Confusion matrix :
			1.1 TP - true positive ,TN - true negative ,  FN - false negative (type 2 error ), FP - False Positive (type 1 error) 
		2. Accuracy 	= (TP+TN) / Total 
		3. Precision 	= TP/ (TP+FP) - How good was my "YES" predictions
		4. RECALL - Sensitivity 	= TP/ (TP+FN) - "How good I was at predicting Actual YES events"
		5. RECALL - Specificity 	= TN/ (TN+FP) - "How good I was at predicting Actual NO events"
		6. F1 score
		7. How to choose !
		8. ROC curves and AUC
		9. Scikit :
			9.1 confusion_matrix(actual,predictions)

	Frameworks
		1. NumPy
		2. SciPy
		3. Pandas
		4. Matplotlib
	Norm
		1. L1 | L2
	Continuous 	and Categorical Variables , binary variables
	DATA PREPERATION - Standerize data 
		1. Mean, Avg
		2. Variance : Bessel correction
		3. Standart deviation ( " how much the numbers jump arond" )
		4. Continuous
			4.1.1	normalize all features
				4.1.1 preprocessing.scale
				4.1.2 the mean is (almost) zero and std is one
				4.1.3 Z-score is the expression of a value in terms of the number of standard deviations from the mean
		5. Categorical Variables
			5.1 Categorical data need to moduled as numbers ( Encoding scheme )
				5.1.1 One hot encoding represent Categorical data as vector with 1 on the current value
				5.1.2 preprocessing.LabelEncoder , le.fit_transform , le.clasess_
				5.1.3 One hot represention : getDummies 
		6.	Text data
			6.1 Word Embedding
				6.1.1 	One hot (NOT SO GOOD)
						6.1.1.1 getDummies
				6.1.2	Frequency based 
					6.1.2.1 count vectors
						6.1.2.1.1 Express each review as a frequency of the words which appear in that review
						6.1.2.1.2 Hash encoding
						
					6.1.2.2 TF-IDF
						6.1.2.2.1 	how often the particular word occures in the document as well in the entire corpuse
						6.1.2.2.2	Represents each word as  numeric data , aggregate as a tensor (metrix)
						6.1.2.2.3	Frequently in  a single document  - might be important
						6.1.2.2.4	Frequently in  a corpuse   - might be a common word
						6.1.2.2.5	xi = xi,j = tf(wi, dj) x idf(wi, D)
						6.1.2.2.6	Tf = Term Frequency; Idf = Inverse Document Frequency ; dj - one document in corpuse; D - entire corpuse
									Measure of how frequently word i occurs in document j
									Measure of how infrequently word i occurs in corpus D
					6.1.2.3 HashingVectorizer
						
				6.1.3	Prediction based 
				6.1.4   Scikit
					6.1.4.1 	vectorize.fit
					6.1.4.2 	vectorize.transform
					6.1.4.3 	vectorize.fit_and_transform
					6.1.4.3 	vectorizer = HashingVectorizer(n_features=8) | vectorizer = CountVectorizer() | vectorizer = TfidfVectorizer()
					
		6.	Image data
			6.1 OopenCV 
			6.2 Image feature extraction for color and grayscale images
			6.3 load as color or grayscale (single channel)
			6.4 flatten , shape 
			6.5 MNIST dataset (handwritting)
		7.  Missing data 
			7.1 auto_data = auto_data.replace('?', np.nan)
			7.2 titanic_df[titanic_df.isnull().any(axis=1)].count()
			7.3 titanic_df.dropna()
Scikit functions 
			8.1 describe
			8.2 to_numeric ( pd.to_numeric(auto_data['horsepower'], errors='coerce') )
			8.3 drop  
			8.4 replace
			8.5 getDummies
			8.6 dropNA
			8.7 isNull (titanic_df[titanic_df.isnull().any(axis=1)].count())
			8.8 X_train, x_test, Y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)
				8.8.1 X - features vector
				8.8.2 Y - labels vector
			8.9 head (10)
			8.10 target
			8.11 clf_svc_pipeline = Pipeline([
										('vect', CountVectorizer()),
										('tfidf', TfidfTransformer()),
										('clf',LinearSVC(penalty="l2", dual=False, tol=0.001))
									])
			8.12 accuracy_score
							from sklearn.metrics import accuracy_score
							acc_svm = accuracy_score(twenty_test.target, predicted)
			8.13 SVM
				8.13.1 from sklearn.svm import LinearSVC
						clf_svm = LinearSVC(penalty="l2", dual=False, tol=1e-5)
						clf_svm.fit(X_train, Y_train)
						
				8.13.2 GridSearchCV :  Exhaustive search over specified parameter values for the estimator to find the best model.
					8.13.2.1  
				